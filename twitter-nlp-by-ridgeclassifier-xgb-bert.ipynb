{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Introduction\n","\n","**This notebook linear model part is based on the tutorial notebook**\n","\n","https://www.kaggle.com/philculliton/nlp-getting-started-tutorial\n","\n","**The sections for RidgeClassifier and XGBClassifier do not contribute to the final score but they are alternative models for this problem**\n","\n","**The BERT section contributes to the final score**"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-11-06T22:50:09.995101Z","iopub.status.busy":"2023-11-06T22:50:09.994842Z","iopub.status.idle":"2023-11-06T22:50:24.894980Z","shell.execute_reply":"2023-11-06T22:50:24.893902Z","shell.execute_reply.started":"2023-11-06T22:50:09.995077Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import string\n","import re\n","from collections import Counter\n","from sklearn import feature_extraction, linear_model, model_selection, preprocessing, decomposition\n","import xgboost as xgb \n","\n","import IPython\n","import contractions\n","from datetime import datetime"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import nltk\n","# nltk.download('stopwords')\n","# nltk.download('wordnet')"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2023-11-06T22:50:24.897306Z","iopub.status.busy":"2023-11-06T22:50:24.896697Z","iopub.status.idle":"2023-11-06T22:50:24.966549Z","shell.execute_reply":"2023-11-06T22:50:24.965600Z","shell.execute_reply.started":"2023-11-06T22:50:24.897278Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["7613 3263\n"]}],"source":["train_df = pd.read_csv(\"train.csv\")\n","test_df = pd.read_csv(\"test.csv\")\n","print(len(train_df), len(test_df))"]},{"cell_type":"markdown","metadata":{},"source":["# Text preprocessing for linear models"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:10.335689Z","iopub.status.busy":"2021-12-21T03:06:10.335160Z","iopub.status.idle":"2021-12-21T03:06:15.185198Z","shell.execute_reply":"2021-12-21T03:06:15.184490Z","shell.execute_reply.started":"2021-12-21T03:06:10.335649Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/dl/_c3b_8811wzgf3pj82f7kl100000gn/T/ipykernel_9229/3514884118.py:46: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[\"clean_text\"] = cleaned_text\n","/var/folders/dl/_c3b_8811wzgf3pj82f7kl100000gn/T/ipykernel_9229/3514884118.py:47: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df.clean_text = df.clean_text.replace(\"%20\", \" \")\n"]}],"source":["twt = nltk.tokenize.TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n","stop = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation)\n","stemmer = nltk.stem.PorterStemmer()\n","lemmatizer = nltk.stem.WordNetLemmatizer()\n","# print(stop)\n","\n","def clean_text(df, col='text', normalize='lemmatize', stopwords=True, add_keyword=False, fill_empty='NULL', drop_dupe=False, shuffle=False):\n","    cleaned_text, pos, neg = [], [], []\n","    text_df = df[col]\n","    if drop_dupe:\n","        df = df.drop_duplicates(col)\n","    \n","    try: \n","        targets = df.target\n","    except:\n","        targets = -np.ones(len(df))\n","        \n","    if add_keyword:\n","        df.keyword = df.keyword.str.replace(\"%20\", \" \").fillna(\"\")\n","        text_df = df.text + \" \" + df.keyword\n","    \n","    for (target, text) in zip(targets, text_df):\n","#         print(text)\n","        text = text.lower().split(\" \")\n","        text = [word for word in text if \"http\" not in word]\n","        text = [contractions.fix(word) for word in text]\n","        text = \" \".join(text).lower()\n","        text = re.sub(r'\\d+|#', '', text)\n","        text = twt.tokenize(text)\n","        if stopwords:\n","            text = [word for word in text if word not in stop]\n","        text = [word for word in text if word not in [\"rt\", \"û_\", \"amp\", \"ûª\", \"ûªs\", \"ûò\", \"ûï\", \"ûó\", \"åè\", \"ìñ1\", \"\\x89\", \"...\", \"..\", \"via\"]]\n","        if normalize == 'lemmatize':\n","            text = [lemmatizer.lemmatize(word) for word in text]\n","        if normalize == 'stem':\n","            text = [stemmer.stem(word) for word in text]\n","            \n","        if target == 1: \n","            pos.append(text)\n","        if target == 0: \n","            neg.append(text)\n","        text = \" \".join(text)\n","        cleaned_text.append(text)\n","#         print(text)\n","        \n","    df[\"clean_text\"] = cleaned_text\n","    df.clean_text = df.clean_text.replace(\"%20\", \" \")\n","    if fill_empty != False:\n","        df.loc[df.clean_text.str.len() == 0, 'clean_text'] = fill_empty\n","    if shuffle:\n","        df = df.sample(frac=1)\n","    \n","    return pos, neg, df\n","        \n","pos_text, neg_text, train_df = clean_text(train_df, add_keyword=False, drop_dupe=True, shuffle=True)\n","_, _, test_df = clean_text(test_df, add_keyword=False)\n","pos_text = [item for sublist in pos_text for item in sublist]\n","neg_text = [item for sublist in neg_text for item in sublist]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:15.186645Z","iopub.status.busy":"2021-12-21T03:06:15.186327Z","iopub.status.idle":"2021-12-21T03:06:15.218148Z","shell.execute_reply":"2021-12-21T03:06:15.217441Z","shell.execute_reply.started":"2021-12-21T03:06:15.186608Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>fire</td>\n","      <td>193</td>\n","      <td>like</td>\n","      <td>203</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>like</td>\n","      <td>145</td>\n","      <td>get</td>\n","      <td>171</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>people</td>\n","      <td>114</td>\n","      <td>fire</td>\n","      <td>159</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>news</td>\n","      <td>99</td>\n","      <td>new</td>\n","      <td>129</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>new</td>\n","      <td>97</td>\n","      <td>body</td>\n","      <td>118</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>building</td>\n","      <td>87</td>\n","      <td>one</td>\n","      <td>115</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>one</td>\n","      <td>86</td>\n","      <td>would</td>\n","      <td>114</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>get</td>\n","      <td>80</td>\n","      <td>time</td>\n","      <td>109</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>disaster</td>\n","      <td>78</td>\n","      <td>video</td>\n","      <td>101</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>flood</td>\n","      <td>76</td>\n","      <td>people</td>\n","      <td>99</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>police</td>\n","      <td>75</td>\n","      <td>u</td>\n","      <td>89</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>would</td>\n","      <td>75</td>\n","      <td>suicide</td>\n","      <td>88</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>storm</td>\n","      <td>75</td>\n","      <td>emergency</td>\n","      <td>87</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>video</td>\n","      <td>73</td>\n","      <td>going</td>\n","      <td>86</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>emergency</td>\n","      <td>72</td>\n","      <td>want</td>\n","      <td>84</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>u</td>\n","      <td>71</td>\n","      <td>news</td>\n","      <td>84</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>year</td>\n","      <td>70</td>\n","      <td>know</td>\n","      <td>83</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>crash</td>\n","      <td>69</td>\n","      <td>disaster</td>\n","      <td>83</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>burning</td>\n","      <td>69</td>\n","      <td>still</td>\n","      <td>82</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>go</td>\n","      <td>66</td>\n","      <td>day</td>\n","      <td>79</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            0    1          0    1\n","0        fire  193       like  203\n","1        like  145        get  171\n","2      people  114       fire  159\n","3        news   99        new  129\n","4         new   97       body  118\n","5    building   87        one  115\n","6         one   86      would  114\n","7         get   80       time  109\n","8    disaster   78      video  101\n","9       flood   76     people   99\n","10     police   75          u   89\n","11      would   75    suicide   88\n","12      storm   75  emergency   87\n","13      video   73      going   86\n","14  emergency   72       want   84\n","15          u   71       news   84\n","16       year   70       know   83\n","17      crash   69   disaster   83\n","18    burning   69      still   82\n","19         go   66        day   79"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["pos_common = pd.DataFrame(Counter(pos_text).most_common(20))\n","neg_common = pd.DataFrame(Counter(neg_text).most_common(20))\n","pd.concat([pos_common, neg_common], axis=1)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:15.220868Z","iopub.status.busy":"2021-12-21T03:06:15.220329Z","iopub.status.idle":"2021-12-21T03:06:15.244339Z","shell.execute_reply":"2021-12-21T03:06:15.243725Z","shell.execute_reply.started":"2021-12-21T03:06:15.220833Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>clean_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4573</th>\n","      <td>6501</td>\n","      <td>injuries</td>\n","      <td>Toronto</td>\n","      <td>Peel police say male cyclist struck near South...</td>\n","      <td>1</td>\n","      <td>NULL</td>\n","    </tr>\n","    <tr>\n","      <th>6866</th>\n","      <td>9838</td>\n","      <td>trauma</td>\n","      <td>Nashville, TN</td>\n","      <td>Esteemed journalist recalls tragic effects of ...</td>\n","      <td>1</td>\n","      <td>NULL</td>\n","    </tr>\n","    <tr>\n","      <th>6694</th>\n","      <td>9591</td>\n","      <td>thunder</td>\n","      <td>NaN</td>\n","      <td>My brother is crying cause the thunder lmao</td>\n","      <td>0</td>\n","      <td>NULL</td>\n","    </tr>\n","    <tr>\n","      <th>6702</th>\n","      <td>9600</td>\n","      <td>thunder</td>\n","      <td>Macon, GA</td>\n","      <td>#thunder outside my house this afternoon #gawx...</td>\n","      <td>1</td>\n","      <td>NULL</td>\n","    </tr>\n","    <tr>\n","      <th>6718</th>\n","      <td>9620</td>\n","      <td>thunderstorm</td>\n","      <td>El Dorado, Arkansas</td>\n","      <td>NWS has Continued a Severe Thunderstorm Warnin...</td>\n","      <td>1</td>\n","      <td>NULL</td>\n","    </tr>\n","    <tr>\n","      <th>6697</th>\n","      <td>9594</td>\n","      <td>thunder</td>\n","      <td>NaN</td>\n","      <td>thunder is legit</td>\n","      <td>0</td>\n","      <td>NULL</td>\n","    </tr>\n","    <tr>\n","      <th>6720</th>\n","      <td>9627</td>\n","      <td>thunderstorm</td>\n","      <td>Asheville, NC</td>\n","      <td>iNWS Alert SPSGSP from 8/5/2015 10:40 PM to 11...</td>\n","      <td>1</td>\n","      <td>NULL</td>\n","    </tr>\n","    <tr>\n","      <th>6723</th>\n","      <td>9634</td>\n","      <td>thunderstorm</td>\n","      <td>73101</td>\n","      <td>Severe Weather Statement issued August 05 at 9...</td>\n","      <td>1</td>\n","      <td>NULL</td>\n","    </tr>\n","    <tr>\n","      <th>6726</th>\n","      <td>9639</td>\n","      <td>thunderstorm</td>\n","      <td>Oklahoma City</td>\n","      <td>Severe Thunderstorm Warning for Oklahoma Count...</td>\n","      <td>1</td>\n","      <td>NULL</td>\n","    </tr>\n","    <tr>\n","      <th>3977</th>\n","      <td>5653</td>\n","      <td>flooding</td>\n","      <td>Ocean City, NJ</td>\n","      <td>Residents in the central part of Ocean City he...</td>\n","      <td>1</td>\n","      <td>NULL</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        id       keyword             location  \\\n","4573  6501      injuries              Toronto   \n","6866  9838        trauma        Nashville, TN   \n","6694  9591       thunder                  NaN   \n","6702  9600       thunder            Macon, GA   \n","6718  9620  thunderstorm  El Dorado, Arkansas   \n","6697  9594       thunder                  NaN   \n","6720  9627  thunderstorm        Asheville, NC   \n","6723  9634  thunderstorm                73101   \n","6726  9639  thunderstorm        Oklahoma City   \n","3977  5653      flooding       Ocean City, NJ   \n","\n","                                                   text  target clean_text  \n","4573  Peel police say male cyclist struck near South...       1       NULL  \n","6866  Esteemed journalist recalls tragic effects of ...       1       NULL  \n","6694        My brother is crying cause the thunder lmao       0       NULL  \n","6702  #thunder outside my house this afternoon #gawx...       1       NULL  \n","6718  NWS has Continued a Severe Thunderstorm Warnin...       1       NULL  \n","6697                                   thunder is legit       0       NULL  \n","6720  iNWS Alert SPSGSP from 8/5/2015 10:40 PM to 11...       1       NULL  \n","6723  Severe Weather Statement issued August 05 at 9...       1       NULL  \n","6726  Severe Thunderstorm Warning for Oklahoma Count...       1       NULL  \n","3977  Residents in the central part of Ocean City he...       1       NULL  "]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>clean_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>13</th>\n","      <td>43</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>What if?!</td>\n","      <td>NULL</td>\n","    </tr>\n","    <tr>\n","      <th>2853</th>\n","      <td>9461</td>\n","      <td>terrorism</td>\n","      <td>NaN</td>\n","      <td>Truth...\\nhttps://t.co/GLzggDjQeH\\n#News\\n#BBC...</td>\n","      <td>NULL</td>\n","    </tr>\n","    <tr>\n","      <th>2860</th>\n","      <td>9478</td>\n","      <td>terrorism</td>\n","      <td>NaN</td>\n","      <td>Truth...\\nhttps://t.co/Kix1j4ZyGx\\n#News\\n#BBC...</td>\n","      <td>NULL</td>\n","    </tr>\n","    <tr>\n","      <th>2863</th>\n","      <td>9487</td>\n","      <td>terrorism</td>\n","      <td>NaN</td>\n","      <td>Truth...\\nhttps://t.co/n1K5nlib9X\\n#News\\n#BBC...</td>\n","      <td>NULL</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        id    keyword location  \\\n","13      43        NaN      NaN   \n","2853  9461  terrorism      NaN   \n","2860  9478  terrorism      NaN   \n","2863  9487  terrorism      NaN   \n","\n","                                                   text clean_text  \n","13                                            What if?!       NULL  \n","2853  Truth...\\nhttps://t.co/GLzggDjQeH\\n#News\\n#BBC...       NULL  \n","2860  Truth...\\nhttps://t.co/Kix1j4ZyGx\\n#News\\n#BBC...       NULL  \n","2863  Truth...\\nhttps://t.co/n1K5nlib9X\\n#News\\n#BBC...       NULL  "]},"metadata":{},"output_type":"display_data"}],"source":["display(train_df.loc[(train_df.clean_text == \"NULL\"), :])\n","display(test_df.loc[(test_df.clean_text == \"NULL\"), :])"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:15.245780Z","iopub.status.busy":"2021-12-21T03:06:15.245475Z","iopub.status.idle":"2021-12-21T03:06:15.269226Z","shell.execute_reply":"2021-12-21T03:06:15.268644Z","shell.execute_reply.started":"2021-12-21T03:06:15.245736Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>clean_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3921</th>\n","      <td>5577</td>\n","      <td>flood</td>\n","      <td>New York</td>\n","      <td>Spot Flood Combo 53inch 300W Curved Cree LED W...</td>\n","      <td>0</td>\n","      <td>selmo catching flame going witnessing slaughter</td>\n","    </tr>\n","    <tr>\n","      <th>4922</th>\n","      <td>7010</td>\n","      <td>mayhem</td>\n","      <td>NaN</td>\n","      <td>Mayhem is beautiful</td>\n","      <td>0</td>\n","      <td>debatequestionswewanttohear saudi arabia israe...</td>\n","    </tr>\n","    <tr>\n","      <th>7596</th>\n","      <td>10851</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>RT @LivingSafely: #NWS issues Severe #Thunders...</td>\n","      <td>1</td>\n","      <td>wreck road blockage woodward avenue northbound...</td>\n","    </tr>\n","    <tr>\n","      <th>1194</th>\n","      <td>1720</td>\n","      <td>bridge%20collapse</td>\n","      <td>Mumbai</td>\n","      <td>@ameenshaikh3 sir i just only wanted to make a...</td>\n","      <td>1</td>\n","      <td>ash australia collapse trent bridge among wors...</td>\n","    </tr>\n","    <tr>\n","      <th>514</th>\n","      <td>740</td>\n","      <td>attacked</td>\n","      <td>SÌ£o Paulo SP,  Brasil</td>\n","      <td>Christian Attacked by Muslims at the Temple Mo...</td>\n","      <td>1</td>\n","      <td>christian attacked muslim temple mount waving ...</td>\n","    </tr>\n","    <tr>\n","      <th>2499</th>\n","      <td>3589</td>\n","      <td>desolate</td>\n","      <td>Macclesfield</td>\n","      <td>@booksbyRoger TY for the follow Go To http://t...</td>\n","      <td>0</td>\n","      <td>green line derailment concern track look like ...</td>\n","    </tr>\n","    <tr>\n","      <th>6147</th>\n","      <td>8768</td>\n","      <td>siren</td>\n","      <td>NaN</td>\n","      <td>I added a video to a @YouTube playlist http://...</td>\n","      <td>0</td>\n","      <td>gaping sinkhole open brooklyn new york</td>\n","    </tr>\n","    <tr>\n","      <th>1578</th>\n","      <td>2279</td>\n","      <td>cliff%20fall</td>\n","      <td>Abuja, Nigeria</td>\n","      <td>When you're in deep sleep and then you dream y...</td>\n","      <td>0</td>\n","      <td>go ibiza pop ah pill get drunk fall cliff real...</td>\n","    </tr>\n","    <tr>\n","      <th>1273</th>\n","      <td>1835</td>\n","      <td>burned</td>\n","      <td>Alabama</td>\n","      <td>Alton brown just did a livestream and he burne...</td>\n","      <td>0</td>\n","      <td>fire burning pendleton burned acre smoke repor...</td>\n","    </tr>\n","    <tr>\n","      <th>419</th>\n","      <td>608</td>\n","      <td>arsonist</td>\n","      <td>ss</td>\n","      <td>@Casper_rmg u on dick</td>\n","      <td>0</td>\n","      <td>minor citation possesion decriminalized substa...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id            keyword                location  \\\n","3921   5577              flood                New York   \n","4922   7010             mayhem                     NaN   \n","7596  10851                NaN                     NaN   \n","1194   1720  bridge%20collapse                  Mumbai   \n","514     740           attacked  SÌ£o Paulo SP,  Brasil   \n","2499   3589           desolate            Macclesfield   \n","6147   8768              siren                     NaN   \n","1578   2279       cliff%20fall          Abuja, Nigeria   \n","1273   1835             burned                 Alabama   \n","419     608           arsonist                      ss   \n","\n","                                                   text  target  \\\n","3921  Spot Flood Combo 53inch 300W Curved Cree LED W...       0   \n","4922                                Mayhem is beautiful       0   \n","7596  RT @LivingSafely: #NWS issues Severe #Thunders...       1   \n","1194  @ameenshaikh3 sir i just only wanted to make a...       1   \n","514   Christian Attacked by Muslims at the Temple Mo...       1   \n","2499  @booksbyRoger TY for the follow Go To http://t...       0   \n","6147  I added a video to a @YouTube playlist http://...       0   \n","1578  When you're in deep sleep and then you dream y...       0   \n","1273  Alton brown just did a livestream and he burne...       0   \n","419                               @Casper_rmg u on dick       0   \n","\n","                                             clean_text  \n","3921    selmo catching flame going witnessing slaughter  \n","4922  debatequestionswewanttohear saudi arabia israe...  \n","7596  wreck road blockage woodward avenue northbound...  \n","1194  ash australia collapse trent bridge among wors...  \n","514   christian attacked muslim temple mount waving ...  \n","2499  green line derailment concern track look like ...  \n","6147             gaping sinkhole open brooklyn new york  \n","1578  go ibiza pop ah pill get drunk fall cliff real...  \n","1273  fire burning pendleton burned acre smoke repor...  \n","419   minor citation possesion decriminalized substa...  "]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>clean_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2370</th>\n","      <td>7922</td>\n","      <td>rainstorm</td>\n","      <td>NaN</td>\n","      <td>@Robot_Rainstorm I'm interested.  Is it throug...</td>\n","      <td>interested yahoo</td>\n","    </tr>\n","    <tr>\n","      <th>1547</th>\n","      <td>5167</td>\n","      <td>fatalities</td>\n","      <td>Greenville, SC</td>\n","      <td>Highway Patrol reports uptick in statewide ped...</td>\n","      <td>highway patrol report uptick statewide pedestr...</td>\n","    </tr>\n","    <tr>\n","      <th>2163</th>\n","      <td>7243</td>\n","      <td>natural%20disaster</td>\n","      <td>NaN</td>\n","      <td>TRAIN ACCIDENT IN HARDA (M.P.) IS NOT NATURAL ...</td>\n","      <td>train accident harda p natural disaster show f...</td>\n","    </tr>\n","    <tr>\n","      <th>1254</th>\n","      <td>4125</td>\n","      <td>drought</td>\n","      <td>NaN</td>\n","      <td>Thought it was a drought!</td>\n","      <td>thought drought</td>\n","    </tr>\n","    <tr>\n","      <th>2878</th>\n","      <td>9530</td>\n","      <td>terrorist</td>\n","      <td>Bharat.</td>\n","      <td>I always wonder why and how media is capable t...</td>\n","      <td>always wonder medium capable meet get detail t...</td>\n","    </tr>\n","    <tr>\n","      <th>2661</th>\n","      <td>8889</td>\n","      <td>smoke</td>\n","      <td>ig: j.nessaa</td>\n","      <td>smoke me out</td>\n","      <td>smoke</td>\n","    </tr>\n","    <tr>\n","      <th>913</th>\n","      <td>3002</td>\n","      <td>dead</td>\n","      <td>Colwyn Bay, Wales</td>\n","      <td>WHAT AN INCREDIBLE CHARACTER MY HEART IS BROKE...</td>\n","      <td>incredible character heart broken actually dea...</td>\n","    </tr>\n","    <tr>\n","      <th>635</th>\n","      <td>2072</td>\n","      <td>casualty</td>\n","      <td>NaN</td>\n","      <td>ÛÏ@MacCocktail: 'The first casualty of war is...</td>\n","      <td>first casualty war truth hiram johnson died da...</td>\n","    </tr>\n","    <tr>\n","      <th>3124</th>\n","      <td>10356</td>\n","      <td>weapons</td>\n","      <td>NaN</td>\n","      <td>Aug 3 1915ÛÓKILL 10000 WITH ROCKS.; Italians ...</td>\n","      <td>aug ûókill rock italian make good use nature's...</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>90</td>\n","      <td>ablaze</td>\n","      <td>121 N La Salle St, Suite 500</td>\n","      <td>'Burning Rahm': Let's hope City Hall builds a ...</td>\n","      <td>burning rahm let u hope city hall build giant ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id             keyword                      location  \\\n","2370   7922           rainstorm                           NaN   \n","1547   5167          fatalities                Greenville, SC   \n","2163   7243  natural%20disaster                           NaN   \n","1254   4125             drought                           NaN   \n","2878   9530           terrorist                       Bharat.   \n","2661   8889               smoke                  ig: j.nessaa   \n","913    3002                dead             Colwyn Bay, Wales   \n","635    2072            casualty                           NaN   \n","3124  10356             weapons                           NaN   \n","27       90              ablaze  121 N La Salle St, Suite 500   \n","\n","                                                   text  \\\n","2370  @Robot_Rainstorm I'm interested.  Is it throug...   \n","1547  Highway Patrol reports uptick in statewide ped...   \n","2163  TRAIN ACCIDENT IN HARDA (M.P.) IS NOT NATURAL ...   \n","1254                          Thought it was a drought!   \n","2878  I always wonder why and how media is capable t...   \n","2661                                       smoke me out   \n","913   WHAT AN INCREDIBLE CHARACTER MY HEART IS BROKE...   \n","635   ÛÏ@MacCocktail: 'The first casualty of war is...   \n","3124  Aug 3 1915ÛÓKILL 10000 WITH ROCKS.; Italians ...   \n","27    'Burning Rahm': Let's hope City Hall builds a ...   \n","\n","                                             clean_text  \n","2370                                   interested yahoo  \n","1547  highway patrol report uptick statewide pedestr...  \n","2163  train accident harda p natural disaster show f...  \n","1254                                    thought drought  \n","2878  always wonder medium capable meet get detail t...  \n","2661                                              smoke  \n","913   incredible character heart broken actually dea...  \n","635   first casualty war truth hiram johnson died da...  \n","3124  aug ûókill rock italian make good use nature's...  \n","27    burning rahm let u hope city hall build giant ...  "]},"metadata":{},"output_type":"display_data"}],"source":["display(train_df.sample(frac=1).head(10))\n","display(test_df.sample(frac=1).head(10))"]},{"cell_type":"markdown","metadata":{},"source":["# Count and Vectorize approach (1-gram)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:15.270625Z","iopub.status.busy":"2021-12-21T03:06:15.270385Z","iopub.status.idle":"2021-12-21T03:06:15.316519Z","shell.execute_reply":"2021-12-21T03:06:15.315835Z","shell.execute_reply.started":"2021-12-21T03:06:15.270593Z"},"trusted":true},"outputs":[],"source":["feature_col = \"clean_text\"\n","\n","count_vectorizer = feature_extraction.text.CountVectorizer()\n","count_vectorizer_sw = feature_extraction.text.CountVectorizer()\n","tfidf = feature_extraction.text.TfidfVectorizer()\n","LSA = decomposition.TruncatedSVD(n_components=100)\n","\n","## let's get counts for the first 5 tweets in the data\n","example_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:50])\n","example_train_vectors_sw = count_vectorizer_sw.fit_transform(train_df[feature_col][0:50])\n","example_tfidf = tfidf.fit_transform(train_df[feature_col][0:50])\n","example_tfidf_lsa = LSA.fit_transform(example_tfidf)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:15.323024Z","iopub.status.busy":"2021-12-21T03:06:15.320907Z","iopub.status.idle":"2021-12-21T03:06:15.334662Z","shell.execute_reply":"2021-12-21T03:06:15.333686Z","shell.execute_reply.started":"2021-12-21T03:06:15.322982Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["No cleaning\n","(1, 505)\n","Cleaned\n","(1, 350)\n","TF-IDF cleaned\n","(1, 350)\n","TF-IDF + LSA cleaned\n","(50,)\n"]}],"source":["## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\n","print('No cleaning')\n","print(example_train_vectors[0].todense().shape)\n","# print(example_train_vectors[0].todense())\n","print('Cleaned')\n","print(example_train_vectors_sw[0].todense().shape)\n","# print(example_train_vectors_sw[0].todense())\n","print('TF-IDF cleaned')\n","print(example_tfidf[0].todense().shape)\n","# print(example_tfidf[0].todense())\n","print('TF-IDF + LSA cleaned')\n","print(example_tfidf_lsa[0].shape)\n","# print(example_tfidf_lsa[0])"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:15.336419Z","iopub.status.busy":"2021-12-21T03:06:15.335950Z","iopub.status.idle":"2021-12-21T03:06:15.860601Z","shell.execute_reply":"2021-12-21T03:06:15.859846Z","shell.execute_reply.started":"2021-12-21T03:06:15.336318Z"},"trusted":true},"outputs":[],"source":["train_vectors = count_vectorizer.fit_transform(train_df[feature_col])\n","train_vectors_sw = count_vectorizer_sw.fit_transform(train_df[feature_col])\n","train_tfidf = tfidf.fit_transform(train_df[feature_col])\n","\n","## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n","# that the tokens in the train vectors are the only ones mapped to the test vectors - \n","# i.e. that the train and test vectors use the same set of tokens.\n","test_vectors = count_vectorizer.transform(test_df[feature_col])\n","test_vectors_sw = count_vectorizer_sw.transform(test_df[feature_col])\n","test_tfidf = tfidf.transform(test_df[feature_col])"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:15.862269Z","iopub.status.busy":"2021-12-21T03:06:15.862008Z","iopub.status.idle":"2021-12-21T03:06:16.530644Z","shell.execute_reply":"2021-12-21T03:06:16.529798Z","shell.execute_reply.started":"2021-12-21T03:06:15.862235Z"},"trusted":true},"outputs":[],"source":["train_tfidf_lsa = LSA.fit_transform(train_tfidf)\n","test_tfidf_lsa = LSA.transform(test_tfidf)"]},{"cell_type":"markdown","metadata":{},"source":["# **Linear Model: Ridge Classifier**"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:16.532700Z","iopub.status.busy":"2021-12-21T03:06:16.532187Z","iopub.status.idle":"2021-12-21T03:06:16.536948Z","shell.execute_reply":"2021-12-21T03:06:16.536051Z","shell.execute_reply.started":"2021-12-21T03:06:16.532662Z"},"trusted":true},"outputs":[],"source":["# clf = linear_model.RidgeClassifier(class_weight='balanced')\n","# ridge_params = {\n","#     \"alpha\": np.linspace(0, 2, 100),\n","#     \"tol\": np.linspace(1e-5, 1e-1, 2000)\n","# }\n","# ridge_rscv = model_selection.RandomizedSearchCV(clf, ridge_params, scoring=[\"f1\", \"precision\", \"recall\"], refit=\"f1\", cv=5, n_iter=100)\n","# ridge_rscv_lsa = model_selection.RandomizedSearchCV(clf, ridge_params, scoring=[\"f1\", \"precision\", \"recall\"], refit=\"f1\", cv=5, n_iter=100)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:16.539205Z","iopub.status.busy":"2021-12-21T03:06:16.538567Z","iopub.status.idle":"2021-12-21T03:06:16.549244Z","shell.execute_reply":"2021-12-21T03:06:16.548444Z","shell.execute_reply.started":"2021-12-21T03:06:16.539167Z"},"trusted":true},"outputs":[],"source":["# search = ridge_rscv.fit(train_tfidf, train_df[\"target\"])\n","# search_lsa = ridge_rscv_lsa.fit(train_tfidf_lsa, train_df[\"target\"])\n","\n","# print(\"Best RidgeClassifier TF-IDF\")\n","# print(search.best_score_)\n","# print(search.best_params_)\n","# print(\"Best RidgeClassifier TF-IDF LSA\")\n","# print(search_lsa.best_score_)\n","# print(search_lsa.best_params_)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:16.551276Z","iopub.status.busy":"2021-12-21T03:06:16.550469Z","iopub.status.idle":"2021-12-21T03:06:16.558067Z","shell.execute_reply":"2021-12-21T03:06:16.557236Z","shell.execute_reply.started":"2021-12-21T03:06:16.551235Z"},"trusted":true},"outputs":[],"source":["# scores_tfidf = model_selection.cross_validate(clf, train_tfidf, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n","\n","# scores_tfidf_lsa = model_selection.cross_validate(clf, train_tfidf_lsa, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n","\n","# print(\"RidgeClassifier TF-IDF F1:              \", scores_tfidf['test_f1'])\n","# print('RidgeClassifier TF-IDF & LSA F1:        ', scores_tfidf_lsa['test_f1'])\n","# print(\"RidgeClassifier TF-IDF Precision:       \", scores_tfidf['test_precision'])\n","# print('RidgeClassifier TF-IDF & LSA Precision: ', scores_tfidf_lsa['test_precision'])\n","# print('RidgeClassifier TF-IDF Recall:          ',  scores_tfidf['test_recall'])\n","# print('RidgeClassifier TF-IDF & LSA Recall:    ', scores_tfidf_lsa['test_recall'])"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:16.566357Z","iopub.status.busy":"2021-12-21T03:06:16.563369Z","iopub.status.idle":"2021-12-21T03:06:16.570748Z","shell.execute_reply":"2021-12-21T03:06:16.569996Z","shell.execute_reply.started":"2021-12-21T03:06:16.566314Z"},"trusted":true},"outputs":[],"source":["# scores_tfidf = model_selection.cross_validate(ridge_rscv.best_estimator_, train_tfidf, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n","\n","# scores_tfidf_lsa = model_selection.cross_validate(ridge_rscv_lsa.best_estimator_, train_tfidf_lsa, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n","\n","# print(\"Best RidgeClassifier TF-IDF F1:              \", scores_tfidf['test_f1'])\n","# print('Best RidgeClassifier TF-IDF & LSA F1:        ', scores_tfidf_lsa['test_f1'])\n","# print(\"Best RidgeClassifier TF-IDF Precision:       \", scores_tfidf['test_precision'])\n","# print('Best RidgeClassifier TF-IDF & LSA Precision: ', scores_tfidf_lsa['test_precision'])\n","# print('Best RidgeClassifier TF-IDF Recall:          ',  scores_tfidf['test_recall'])\n","# print('Best RidgeClassifier TF-IDF & LSA Recall:    ', scores_tfidf_lsa['test_recall'])"]},{"cell_type":"markdown","metadata":{},"source":["# **GBDT: XGB Classifier**\n","Turns out not as good as ridge"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:16.574003Z","iopub.status.busy":"2021-12-21T03:06:16.573476Z","iopub.status.idle":"2021-12-21T03:06:16.581815Z","shell.execute_reply":"2021-12-21T03:06:16.581102Z","shell.execute_reply.started":"2021-12-21T03:06:16.573964Z"},"trusted":true},"outputs":[],"source":["# xgb_clf = xgb.XGBClassifier(random_state=765, tree_method='gpu_hist', predictor='gpu_predictor')\n","# xgb_params = {\n","#     \"max_depth\": [i for i in range(4, 14)],\n","#     \"min_child_weight\": np.linspace(0.25, 0.45, 100),\n","#     \"gamma\": np.linspace(0, 0.015, 1000),\n","#     \"learning_rate\": np.linspace(0.2, 0.5, 100),\n","# }\n","# xgb_rscv = model_selection.RandomizedSearchCV(xgb_clf, xgb_params, scoring=[\"f1\", \"precision\", \"recall\"], refit=\"f1\", cv=5, verbose=2)\n","# xgb_rscv_lsa = model_selection.RandomizedSearchCV(xgb_clf, xgb_params, scoring=[\"f1\", \"precision\", \"recall\"], refit=\"f1\", cv=5, verbose=2)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:16.584000Z","iopub.status.busy":"2021-12-21T03:06:16.583428Z","iopub.status.idle":"2021-12-21T03:06:16.590777Z","shell.execute_reply":"2021-12-21T03:06:16.590114Z","shell.execute_reply.started":"2021-12-21T03:06:16.583958Z"},"trusted":true},"outputs":[],"source":["# search = xgb_rscv.fit(train_tfidf, train_df[\"target\"])\n","# search_lsa = xgb_rscv_lsa.fit(train_tfidf_lsa, train_df[\"target\"])\n","# IPython.display.clear_output()\n","# print(\"Best XGBClassifier TF-IDF\")\n","# print(search.best_score_)\n","# print(search.best_params_)\n","# print(\"Best XGBClassifier TF-IDF LSA\")\n","# print(search_lsa.best_score_)\n","# print(search_lsa.best_params_)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:16.592708Z","iopub.status.busy":"2021-12-21T03:06:16.592198Z","iopub.status.idle":"2021-12-21T03:06:16.599580Z","shell.execute_reply":"2021-12-21T03:06:16.598886Z","shell.execute_reply.started":"2021-12-21T03:06:16.592671Z"},"trusted":true},"outputs":[],"source":["# scores_tfidf = model_selection.cross_validate(xgb_clf, train_tfidf, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n","# scores_tfidf_lsa = model_selection.cross_validate(xgb_clf, train_tfidf_lsa, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n","\n","# print(\"XGBClassifier TF-IDF F1:              \", scores_tfidf['test_f1'])\n","# print('XGBClassifier TF-IDF & LSA F1:        ', scores_tfidf_lsa['test_f1'])\n","# print(\"XGBClassifier TF-IDF Precision:       \", scores_tfidf['test_precision'])\n","# print('XGBClassifier TF-IDF & LSA Precision: ', scores_tfidf_lsa['test_precision'])\n","# print('XGBClassifier TF-IDF Recall:          ',  scores_tfidf['test_recall'])\n","# print('XGBClassifier TF-IDF & LSA Recall:    ', scores_tfidf_lsa['test_recall'])"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:16.601668Z","iopub.status.busy":"2021-12-21T03:06:16.601165Z","iopub.status.idle":"2021-12-21T03:06:16.610237Z","shell.execute_reply":"2021-12-21T03:06:16.609519Z","shell.execute_reply.started":"2021-12-21T03:06:16.601633Z"},"trusted":true},"outputs":[],"source":["# scores_tfidf = model_selection.cross_validate(xgb_rscv.best_estimator_, train_tfidf, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n","# scores_tfidf_lsa = model_selection.cross_validate(xgb_rscv_lsa.best_estimator_, train_tfidf_lsa, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n","\n","# print(\"Best XGBClassifier TF-IDF F1:              \", scores_tfidf['test_f1'])\n","# print('Best XGBClassifier TF-IDF & LSA F1:        ', scores_tfidf_lsa['test_f1'])\n","# print(\"Best XGBClassifier TF-IDF Precision:       \", scores_tfidf['test_precision'])\n","# print('Best XGBClassifier TF-IDF & LSA Precision: ', scores_tfidf_lsa['test_precision'])\n","# print('Best XGBClassifier TF-IDF Recall:          ',  scores_tfidf['test_recall'])\n","# print('Best XGBClassifier TF-IDF & LSA Recall:    ', scores_tfidf_lsa['test_recall'])"]},{"cell_type":"markdown","metadata":{},"source":["# **MAIN CONTENT: BERT**\n","\n","**Note that this is not the best version, fine tunings and validation may help obtain a better score**"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import AdamW, get_linear_schedule_with_warmup"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:16.612068Z","iopub.status.busy":"2021-12-21T03:06:16.611520Z","iopub.status.idle":"2021-12-21T03:06:18.023738Z","shell.execute_reply":"2021-12-21T03:06:18.023126Z","shell.execute_reply.started":"2021-12-21T03:06:16.612032Z"},"trusted":true},"outputs":[],"source":["bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"markdown","metadata":{},"source":["Adding additional tokens for masking URLs and usernames in tweets"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:18.025328Z","iopub.status.busy":"2021-12-21T03:06:18.025057Z","iopub.status.idle":"2021-12-21T03:06:18.032956Z","shell.execute_reply":"2021-12-21T03:06:18.032197Z","shell.execute_reply.started":"2021-12-21T03:06:18.025290Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Using bos_token, but it is not set yet.\n","Using eos_token, but it is not set yet.\n"]},{"data":{"text/plain":["BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['[LINK]', '[USER]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n","\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n","\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n","\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n","\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n","\t30522: AddedToken(\"[LINK]\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n","\t30523: AddedToken(\"[USER]\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n","}"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["bert_tokenizer.add_special_tokens({'additional_special_tokens': ['[LINK]', '[USER]']})\n","bert_tokenizer"]},{"cell_type":"markdown","metadata":{},"source":["As BERT is able to read complete passages and learn from the context, too much text preprocessing may not be beneficial.\n","\n","Some minor preprocessing with URLs, @usernames, and #hashtag, as they may be tokenized weirdly and the token make no sense\n","\n","*Note: The BERT model still did pretty good without the above processing*"]},{"cell_type":"markdown","metadata":{},"source":["Now tokenize the data"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:11:51.344273Z","iopub.status.busy":"2021-12-21T03:11:51.343989Z","iopub.status.idle":"2021-12-21T03:11:58.264497Z","shell.execute_reply":"2021-12-21T03:11:58.263767Z","shell.execute_reply.started":"2021-12-21T03:11:51.344238Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/Users/chantom/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/Users/chantom/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["def bert_tokenize(df, tokenizer=bert_tokenizer, max_seq_len = 100):\n","    input_sequences = []\n","    # The attention mask is an optional argument used when batching sequences together.\n","    # The attention mask is a binary tensor indicating the position of the padded indices so that the model does not attend to them.\n","    attention_masks = []\n","    bert_text = []\n","    \n","    # some very minor text processing, try to keep the text as close as original\n","    for i, text in enumerate(df['text']):\n","#         print(i, text)\n","        text = text.replace(\"\\n\", \" \").split(\" \")\n","        text = [word if \"http\" not in word else \"[LINK]\" for word in text]\n","        text = [word if \"@\" not in word else \"[USER]\" for word in text]\n","        text = \" \".join(text)\n","        text = re.sub(r'#', '', text)\n","        bert_text.append(text)\n","        \n","#         print(i, text)\n","        sequence_dict = tokenizer.encode_plus(text, max_length=max_seq_len, pad_to_max_length=True)\n","        input_ids = sequence_dict['input_ids']\n","        att_mask = sequence_dict['attention_mask']\n","#         print(i, tokenizer.tokenize(text))\n","        input_sequences.append(input_ids)\n","        attention_masks.append(att_mask)\n","    \n","    df['bert_text'] = bert_text\n","    return input_sequences, attention_masks, df\n","\n","train_X, train_att, train_df = bert_tokenize(train_df)\n","train_y = train_df['target'].values\n","test_X, test_att, test_df = bert_tokenize(test_df)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:25.171033Z","iopub.status.busy":"2021-12-21T03:06:25.170787Z","iopub.status.idle":"2021-12-21T03:06:25.176826Z","shell.execute_reply":"2021-12-21T03:06:25.176081Z","shell.execute_reply.started":"2021-12-21T03:06:25.170999Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[101, 11503, 11733, 1005, 1055, 2725, 1037, 3582, 11867, 9910, 3892, 10047, 7491, 1997, 8404, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[101, 2074, 3047, 1037, 6659, 2482, 5823, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}],"source":["# Checking the tokenized format\n","print(train_X[0])\n","print(train_att[0])\n","print(test_X[0])\n","print(test_att[0])"]},{"cell_type":"markdown","metadata":{},"source":["Forming dataset"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["# device = torch.device(\"mps\" if torch.backends.mps.is_available()  else \"cpu\")\n","device = torch.device(\"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:25.178471Z","iopub.status.busy":"2021-12-21T03:06:25.178026Z","iopub.status.idle":"2021-12-21T03:06:25.443672Z","shell.execute_reply":"2021-12-21T03:06:25.442954Z","shell.execute_reply.started":"2021-12-21T03:06:25.178435Z"},"trusted":true},"outputs":[],"source":["train_X = torch.tensor(train_X, device=device)\n","train_y = torch.tensor(train_y, device=device)\n","train_att = torch.tensor(train_att, device=device)\n","test_X = torch.tensor(test_X, device=device)\n","test_att = torch.tensor(test_att, device=device)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:25.445086Z","iopub.status.busy":"2021-12-21T03:06:25.444843Z","iopub.status.idle":"2021-12-21T03:06:25.452889Z","shell.execute_reply":"2021-12-21T03:06:25.450742Z","shell.execute_reply.started":"2021-12-21T03:06:25.445055Z"},"trusted":true},"outputs":[],"source":["batch_size = 32\n","train_data = torch.utils.data.TensorDataset(train_X, train_att, train_y)\n","train_sampler = torch.utils.data.RandomSampler(train_data)\n","train_dataloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","test_data = torch.utils.data.TensorDataset(test_X, test_att)\n","test_sampler = torch.utils.data.SequentialSampler(test_data)\n","test_dataloader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{},"source":["**Pretrained model from bert-base-uncased**\n","\n","resize_token_embeddings is required as we have added new special tokens"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:25.454614Z","iopub.status.busy":"2021-12-21T03:06:25.454314Z","iopub.status.idle":"2021-12-21T03:06:42.064893Z","shell.execute_reply":"2021-12-21T03:06:42.064148Z","shell.execute_reply.started":"2021-12-21T03:06:25.454575Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["Embedding(30524, 768)"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","model.resize_token_embeddings(len(bert_tokenizer))"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:42.066541Z","iopub.status.busy":"2021-12-21T03:06:42.066139Z","iopub.status.idle":"2021-12-21T03:06:46.486553Z","shell.execute_reply":"2021-12-21T03:06:46.485855Z","shell.execute_reply.started":"2021-12-21T03:06:42.066502Z"},"trusted":true},"outputs":[],"source":["model.to(device)\n","IPython.display.clear_output()"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:46.489372Z","iopub.status.busy":"2021-12-21T03:06:46.487639Z","iopub.status.idle":"2021-12-21T03:06:46.499720Z","shell.execute_reply":"2021-12-21T03:06:46.499089Z","shell.execute_reply.started":"2021-12-21T03:06:46.489331Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/chantom/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n","loss_fct = torch.nn.NLLLoss()"]},{"cell_type":"markdown","metadata":{},"source":["Define train and test functions"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:46.502396Z","iopub.status.busy":"2021-12-21T03:06:46.500770Z","iopub.status.idle":"2021-12-21T03:06:46.513941Z","shell.execute_reply":"2021-12-21T03:06:46.513259Z","shell.execute_reply.started":"2021-12-21T03:06:46.502355Z"},"trusted":true},"outputs":[],"source":["def train(epoch):\n","    t0 = datetime.now()\n","    model.train()\n","    for i, batch in enumerate(train_dataloader, start=1):\n","        # batch = tuple(t.to(device) for t in batch)\n","        inputs, att_masks, labels = batch\n","        model.zero_grad()  \n","        \n","        logits = model(inputs, attention_mask=att_masks)\n","        outputs = F.log_softmax(logits[0], dim=1)\n","        \n","        loss = loss_fct(outputs.view(-1, 2), labels.view(-1))\n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        optimizer.step()\n","        \n","        if i % 20 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0%})] - Elapsed: {}  |  Loss: {:.4f}'.format(\n","                epoch, i * len(inputs), len(train_dataloader.dataset),\n","                 i / len(train_dataloader), datetime.now() - t0, loss.item()\n","            ))"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:46.515621Z","iopub.status.busy":"2021-12-21T03:06:46.515238Z","iopub.status.idle":"2021-12-21T03:06:46.524356Z","shell.execute_reply":"2021-12-21T03:06:46.523480Z","shell.execute_reply.started":"2021-12-21T03:06:46.515585Z"},"trusted":true},"outputs":[],"source":["# def test():\n","#     t0 = datetime.now()\n","#     model.eval()\n","#     test_loss, test_acc = 0, 0\n","#     for batch in test_dataloader:\n","#         batch = tuple(t.to(device) for t in batch)\n","#         inputs, att_masks, labels = batch\n","#         with torch.no_grad():\n","#             logits = model(inputs, attention_mask=att_masks)\n","#             outputs = F.log_softmax(logits[0], dim=1)\n","            \n","#             loss = loss_fct(outputs.view(-1, 2), labels.view(-1))\n","\n","#         test_loss += loss.item()\n","#         outputs = outputs.detach().cpu().numpy()\n","\n","#         pred = np.argmax(outputs, axis=1)\n","#         labels = labels.cpu().numpy()\n","        \n","#         test_acc += accuracy_score(pred, labels)\n","\n","#     test_loss /= len(test_dataloader)\n","#     test_acc /= len(test_dataloader)\n","#     print('\\nTest set: Loss: {:.4f}, Accuracy: {:.1%} - Elapsed: {}\\n'.format(\n","#         test_loss, test_acc, datetime.now() - t0\n","#     ))"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:06:46.526351Z","iopub.status.busy":"2021-12-21T03:06:46.525798Z","iopub.status.idle":"2021-12-21T03:09:15.001092Z","shell.execute_reply":"2021-12-21T03:09:14.999923Z","shell.execute_reply.started":"2021-12-21T03:06:46.526314Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Epoch: 0 [640/7503 (9%)] - Elapsed: 0:01:15.628283  |  Loss: 0.5330\n","Train Epoch: 0 [1280/7503 (17%)] - Elapsed: 0:02:32.760750  |  Loss: 0.4231\n","Train Epoch: 0 [1920/7503 (26%)] - Elapsed: 0:03:42.994129  |  Loss: 0.3329\n","Train Epoch: 0 [2560/7503 (34%)] - Elapsed: 0:04:52.932639  |  Loss: 0.4181\n","Train Epoch: 0 [3200/7503 (43%)] - Elapsed: 0:06:06.339276  |  Loss: 0.4685\n","Train Epoch: 0 [3840/7503 (51%)] - Elapsed: 0:07:16.597780  |  Loss: 0.2109\n","Train Epoch: 0 [4480/7503 (60%)] - Elapsed: 0:08:27.245850  |  Loss: 0.3480\n","Train Epoch: 0 [5120/7503 (68%)] - Elapsed: 0:09:37.618052  |  Loss: 0.2152\n","Train Epoch: 0 [5760/7503 (77%)] - Elapsed: 0:15:22.088960  |  Loss: 0.4059\n","Train Epoch: 0 [6400/7503 (85%)] - Elapsed: 0:16:34.623297  |  Loss: 0.4037\n","Train Epoch: 0 [7040/7503 (94%)] - Elapsed: 0:17:50.331853  |  Loss: 0.5258\n"]}],"source":["num_epoch = 1\n","for epoch in range(num_epoch):\n","    train(epoch)\n","#     test()"]},{"cell_type":"markdown","metadata":{},"source":["**Generating predictions for test data**"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:09:15.007110Z","iopub.status.busy":"2021-12-21T03:09:15.005084Z","iopub.status.idle":"2021-12-21T03:09:15.014295Z","shell.execute_reply":"2021-12-21T03:09:15.013264Z","shell.execute_reply.started":"2021-12-21T03:09:15.007063Z"},"trusted":true},"outputs":[],"source":["def predict(text):\n","    # pre-process text\n","    input_ = torch.tensor(bert_tokenizer.encode(text)).unsqueeze(0).to(device)\n","    logits = model.eval()(input_ids=input_)[0]\n","    pred = F.softmax(logits, dim=1)[0]\n","    return pred"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:09:15.015917Z","iopub.status.busy":"2021-12-21T03:09:15.015599Z","iopub.status.idle":"2021-12-21T03:09:55.617804Z","shell.execute_reply":"2021-12-21T03:09:55.617088Z","shell.execute_reply.started":"2021-12-21T03:09:15.015877Z"},"trusted":true},"outputs":[],"source":["predictions = []\n","for text in test_df.text:\n","    prob = predict(text)\n","    pred = np.argmax(prob.cpu().detach().numpy())\n","    predictions.append(pred)"]},{"cell_type":"markdown","metadata":{},"source":["# Submission"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:09:55.619519Z","iopub.status.busy":"2021-12-21T03:09:55.619096Z","iopub.status.idle":"2021-12-21T03:09:55.632407Z","shell.execute_reply":"2021-12-21T03:09:55.631519Z","shell.execute_reply.started":"2021-12-21T03:09:55.619480Z"},"trusted":true},"outputs":[],"source":["sample_submission = pd.read_csv(\"sample_submission.csv\")"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:09:55.633958Z","iopub.status.busy":"2021-12-21T03:09:55.633696Z","iopub.status.idle":"2021-12-21T03:09:55.639764Z","shell.execute_reply":"2021-12-21T03:09:55.639111Z","shell.execute_reply.started":"2021-12-21T03:09:55.633925Z"},"trusted":true},"outputs":[],"source":["# train_prediction = ridge_rscv.best_estimator_.predict(train_tfidf)\n","# train_df['pred_target'] = train_prediction\n","\n","# ridge with rscv\n","# sample_submission[\"target\"] = ridge_rscv.best_estimator_.predict(test_tfidf)\n","\n","# bert\n","sample_submission[\"target\"] = predictions"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:09:55.642015Z","iopub.status.busy":"2021-12-21T03:09:55.641444Z","iopub.status.idle":"2021-12-21T03:09:55.649721Z","shell.execute_reply":"2021-12-21T03:09:55.649000Z","shell.execute_reply.started":"2021-12-21T03:09:55.641973Z"},"trusted":true},"outputs":[],"source":["# clean_text_wc = train_df.clean_text.str.count(' ').add(1)\n","# short_text_incorrect = train_df.loc[(clean_text_wc < 5) & (train_df.target != train_df.pred_target), :]\n","# (short_text_incorrect.target == 1).sum(), (short_text_incorrect.target == 0).sum()"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:12:16.325590Z","iopub.status.busy":"2021-12-21T03:12:16.324994Z","iopub.status.idle":"2021-12-21T03:12:16.348313Z","shell.execute_reply":"2021-12-21T03:12:16.347627Z","shell.execute_reply.started":"2021-12-21T03:12:16.325549Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>target</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>clean_text</th>\n","      <th>bert_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>528</th>\n","      <td>1732</td>\n","      <td>1</td>\n","      <td>buildings%20burning</td>\n","      <td>NaN</td>\n","      <td>@CTVKathyLe and in other news: don't run into ...</td>\n","      <td>news run burning building</td>\n","      <td>[USER] and in other news: don't run into burni...</td>\n","    </tr>\n","    <tr>\n","      <th>1324</th>\n","      <td>4369</td>\n","      <td>1</td>\n","      <td>earthquake</td>\n","      <td>Sydney, New South Wales</td>\n","      <td>#Children traumatised after the Nepal earthqua...</td>\n","      <td>child traumatised nepal earthquake educated co...</td>\n","      <td>Children traumatised after the Nepal earthquak...</td>\n","    </tr>\n","    <tr>\n","      <th>805</th>\n","      <td>2646</td>\n","      <td>0</td>\n","      <td>crashed</td>\n","      <td>USA</td>\n","      <td>Website Malfunctioning? PHP Scripts not workin...</td>\n","      <td>website malfunctioning php script working data...</td>\n","      <td>Website Malfunctioning? PHP Scripts not workin...</td>\n","    </tr>\n","    <tr>\n","      <th>3022</th>\n","      <td>9975</td>\n","      <td>1</td>\n","      <td>tsunami</td>\n","      <td>NaN</td>\n","      <td>When tsunami says your order will take 40 minu...</td>\n","      <td>tsunami say order take minute placed order way</td>\n","      <td>When tsunami says your order will take 40 minu...</td>\n","    </tr>\n","    <tr>\n","      <th>3219</th>\n","      <td>10699</td>\n","      <td>0</td>\n","      <td>wreck</td>\n","      <td>Jackson, MS</td>\n","      <td>Now that IÛªve figured out how to get my musi...</td>\n","      <td>ûªve figured get music rental car take night d...</td>\n","      <td>Now that IÛªve figured out how to get my musi...</td>\n","    </tr>\n","    <tr>\n","      <th>2088</th>\n","      <td>7011</td>\n","      <td>0</td>\n","      <td>mayhem</td>\n","      <td>Hollywood, Ca</td>\n","      <td>Wed Aug 8 !  #Mayhem @ Avalon !  19+ Event ! J...</td>\n","      <td>wed aug mayhem avalon event july aug bdays fre...</td>\n","      <td>Wed Aug 8 !  Mayhem [USER] Avalon !  19+ Event...</td>\n","    </tr>\n","    <tr>\n","      <th>641</th>\n","      <td>2092</td>\n","      <td>0</td>\n","      <td>casualty</td>\n","      <td>NaN</td>\n","      <td>charlie from casualty at the ashes https://t.c...</td>\n","      <td>charlie casualty ash</td>\n","      <td>charlie from casualty at the ashes [LINK]</td>\n","    </tr>\n","    <tr>\n","      <th>205</th>\n","      <td>668</td>\n","      <td>0</td>\n","      <td>attack</td>\n","      <td>Worldwide</td>\n","      <td>Cooper the Super Pooper. The hero dog who save...</td>\n","      <td>cooper super pooper hero dog saved drowning de...</td>\n","      <td>Cooper the Super Pooper. The hero dog who save...</td>\n","    </tr>\n","    <tr>\n","      <th>2968</th>\n","      <td>9821</td>\n","      <td>1</td>\n","      <td>trauma</td>\n","      <td>Nashville, TN</td>\n","      <td>Esteemed journalist recalls tragic effects of ...</td>\n","      <td>esteemed journalist recall tragic effect unadd...</td>\n","      <td>Esteemed journalist recalls tragic effects of ...</td>\n","    </tr>\n","    <tr>\n","      <th>623</th>\n","      <td>2030</td>\n","      <td>1</td>\n","      <td>casualties</td>\n","      <td>Worldwide</td>\n","      <td>Warfighting Robots Could Reduce Civilian Casua...</td>\n","      <td>warfighting robot could reduce civilian casual...</td>\n","      <td>Warfighting Robots Could Reduce Civilian Casua...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id  target              keyword                 location  \\\n","528    1732       1  buildings%20burning                      NaN   \n","1324   4369       1           earthquake  Sydney, New South Wales   \n","805    2646       0              crashed                      USA   \n","3022   9975       1              tsunami                      NaN   \n","3219  10699       0                wreck              Jackson, MS   \n","2088   7011       0               mayhem            Hollywood, Ca   \n","641    2092       0             casualty                      NaN   \n","205     668       0               attack                Worldwide   \n","2968   9821       1               trauma            Nashville, TN   \n","623    2030       1           casualties                Worldwide   \n","\n","                                                   text  \\\n","528   @CTVKathyLe and in other news: don't run into ...   \n","1324  #Children traumatised after the Nepal earthqua...   \n","805   Website Malfunctioning? PHP Scripts not workin...   \n","3022  When tsunami says your order will take 40 minu...   \n","3219  Now that IÛªve figured out how to get my musi...   \n","2088  Wed Aug 8 !  #Mayhem @ Avalon !  19+ Event ! J...   \n","641   charlie from casualty at the ashes https://t.c...   \n","205   Cooper the Super Pooper. The hero dog who save...   \n","2968  Esteemed journalist recalls tragic effects of ...   \n","623   Warfighting Robots Could Reduce Civilian Casua...   \n","\n","                                             clean_text  \\\n","528                           news run burning building   \n","1324  child traumatised nepal earthquake educated co...   \n","805   website malfunctioning php script working data...   \n","3022     tsunami say order take minute placed order way   \n","3219  ûªve figured get music rental car take night d...   \n","2088  wed aug mayhem avalon event july aug bdays fre...   \n","641                                charlie casualty ash   \n","205   cooper super pooper hero dog saved drowning de...   \n","2968  esteemed journalist recall tragic effect unadd...   \n","623   warfighting robot could reduce civilian casual...   \n","\n","                                              bert_text  \n","528   [USER] and in other news: don't run into burni...  \n","1324  Children traumatised after the Nepal earthquak...  \n","805   Website Malfunctioning? PHP Scripts not workin...  \n","3022  When tsunami says your order will take 40 minu...  \n","3219  Now that IÛªve figured out how to get my musi...  \n","2088  Wed Aug 8 !  Mayhem [USER] Avalon !  19+ Event...  \n","641           charlie from casualty at the ashes [LINK]  \n","205   Cooper the Super Pooper. The hero dog who save...  \n","2968  Esteemed journalist recalls tragic effects of ...  \n","623   Warfighting Robots Could Reduce Civilian Casua...  "]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["# display(sample_submission.head(30))\n","# display(test_df['text'].head(30))\n","pd.merge(sample_submission, test_df, on=['id']).sample(frac=1).head(10)"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2021-12-21T03:09:55.678172Z","iopub.status.busy":"2021-12-21T03:09:55.677944Z","iopub.status.idle":"2021-12-21T03:09:55.691385Z","shell.execute_reply":"2021-12-21T03:09:55.690705Z","shell.execute_reply.started":"2021-12-21T03:09:55.678141Z"},"trusted":true},"outputs":[],"source":["sample_submission.to_csv(\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
